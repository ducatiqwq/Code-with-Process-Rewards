# Final Report: Multi-Step Code Synthesis with Intermediate Reward Signals from Execution

Recent advances in test-time scaling have demonstrated that reinforcement learning (RL) with process-level supervision can significantly enhance large language models (LLMs). In this work, we investigate the use of \textit{process rewards}—which provide fine-grained feedback at intermediate steps—for code generation tasks. Leveraging the structured and executable nature of programming, we design a two-stage training pipeline where supervised fine-tuning (SFT) generates code scaffolds and RL fills in implementation details guided by process-level feedback.

Experiments on the LiveCodeBench dataset show that our approach improves training efficiency and final accuracy compared to outcome-only baselines. However, we also observe generalization challenges on the test set, motivating further exploration of robustness techniques. We propose a turn-aware variant of the Generalized Advantage Estimator , named tGAE, to better align with the multi-step nature of code synthesis, and we report several findings on reward design, input perturbation, and reasoning behavior.

Our results highlight both the promise and the complexity of integrating process rewards in LLM training, pointing to future work on scaling, exploration, and general-purpose reward strategies for structured generation tasks.